{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a6a097e0-f312-4d97-9069-ecab9dc99db6",
    "_uuid": "62b5a0a5-cc54-479c-bf03-66fc2a888fa6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const py_version = '3.6.2'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'tabulator': 'https://cdn.jsdelivr.net/npm/tabulator-tables@6.3.0/dist/js/tabulator.min', 'moment': 'https://cdn.jsdelivr.net/npm/luxon/build/global/luxon.min'}, 'shim': {}});\n      require([\"tabulator\"], function(Tabulator) {\n        window.Tabulator = Tabulator\n        on_load()\n      })\n      require([\"moment\"], function(moment) {\n        window.moment = moment\n        on_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 2;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window.Tabulator !== undefined) && (!(window.Tabulator instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.5.4/dist/bundled/datatabulator/tabulator-tables@6.3.0/dist/js/tabulator.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(encodeURI(urls[i]))\n      }\n    }    if (((window.moment !== undefined) && (!(window.moment instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.5.4/dist/bundled/datatabulator/luxon/build/global/luxon.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(encodeURI(urls[i]))\n      }\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.5.4/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.holoviz.org/panel/1.5.4/dist/bundled/datatabulator/tabulator-tables@6.3.0/dist/js/tabulator.min.js\", \"https://cdn.holoviz.org/panel/1.5.4/dist/bundled/datatabulator/luxon/build/global/luxon.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.2.min.js\", \"https://cdn.holoviz.org/panel/1.5.4/dist/panel.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [\"https://cdn.holoviz.org/panel/1.5.4/dist/bundled/datatabulator/tabulator-tables@6.3.0/dist/css/tabulator_simple.min.css?v=1.5.4\"];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n        var NewBokeh = root.Bokeh;\n        if (Bokeh.versions === undefined) {\n          Bokeh.versions = new Map();\n        }\n        if (NewBokeh.version !== Bokeh.version) {\n          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n        }\n        root.Bokeh = Bokeh;\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='7f0c23dc-80ad-47a6-9b79-bc6c3ba70a0a'>\n",
       "  <div id=\"d3aa5ae5-2ea3-4826-ab3c-0fa9f094da97\" data-root-id=\"7f0c23dc-80ad-47a6-9b79-bc6c3ba70a0a\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"757df850-29c2-4d7f-a661-197258796bfe\":{\"version\":\"3.6.2\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"7f0c23dc-80ad-47a6-9b79-bc6c3ba70a0a\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"3b3b653d-d0fa-41f8-9533-0b9c9bb5e73c\",\"attributes\":{\"plot_id\":\"7f0c23dc-80ad-47a6-9b79-bc6c3ba70a0a\",\"comm_id\":\"a53c5a82d9574009ba095f37bbfbe8bc\",\"client_comm_id\":\"6fc4f7c190de489bacddd0e37d44caba\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"JSComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"ReactComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"757df850-29c2-4d7f-a661-197258796bfe\",\"roots\":{\"7f0c23dc-80ad-47a6-9b79-bc6c3ba70a0a\":\"d3aa5ae5-2ea3-4826-ab3c-0fa9f094da97\"},\"root_ids\":[\"7f0c23dc-80ad-47a6-9b79-bc6c3ba70a0a\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined && ( root.Tabulator !== undefined) && ( root.Tabulator !== undefined))\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "7f0c23dc-80ad-47a6-9b79-bc6c3ba70a0a"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from variables import tavily_api, auth_token\n",
    "from huggingface_hub import login\n",
    "login(token=auth_token)\n",
    "import openmeteo_requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "import copy\n",
    "from retry_requests import retry\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional, Literal, Union, Callable, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from tavily import TavilyClient\n",
    "from transformers import T5Tokenizer, T5ForSequenceClassification, T5ForConditionalGeneration\n",
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import faiss\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import langchain\n",
    "\n",
    "from ragclient import RAGClient\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3449e8cfdcef43fa916b4719414fb8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:810: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_save_path = '/workspace/HF_model/'\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            use_auth_token=True,\n",
    "            quantization_config=bnb_config,\n",
    "            cache_dir=model_save_path\n",
    "        ).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\", use_auth_token=True)\n",
    "encoder = AutoModel.from_pretrained('google-bert/bert-base-uncased').to(device)\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
    "model_kwargs = {'device': device}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embedding_model = 'BAAI/bge-base-en-v1.5'\n",
    "embedder = HuggingFaceBgeEmbeddings(model_name=embedding_model,\n",
    "                         model_kwargs = model_kwargs,\n",
    "                         encode_kwargs=encode_kwargs,\n",
    "                         query_instruction=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectiveRAGFilter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        upper_threshold: float = 0.5,\n",
    "        lower_threshold: float = -0.9,\n",
    "        filter_threshold: float = -0.5,\n",
    "        top_n: int = 5,\n",
    "        max_length: int = 512,\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model.to(self.device)\n",
    "        self.upper_threshold = upper_threshold\n",
    "        self.lower_threshold = lower_threshold\n",
    "        self.filter_threshold = filter_threshold\n",
    "        self.max_length = max_length\n",
    "        self.top_n = top_n\n",
    "    \n",
    "    def split_into_passages(self, psg: str, mode: str = \"excerption\") -> List[str]:\n",
    "        if mode == 'fixed_num':\n",
    "            final_strips = []\n",
    "            window_length = 50\n",
    "            words = psg.split(' ')\n",
    "            buf = []\n",
    "            for w in words:\n",
    "                buf.append(w)\n",
    "                if len(buf) == window_length:\n",
    "                    final_strips.append(' '.join(buf))\n",
    "                    buf = []\n",
    "            if buf != []:\n",
    "                if len(buf) < 10:\n",
    "                    final_strips[-1] += (' ' + ' '.join(buf))\n",
    "                else:\n",
    "                    final_strips.append(' '.join(buf))\n",
    "            return final_strips\n",
    "        \n",
    "        if mode == 'excerption':\n",
    "            num_concatenate_strips = 3\n",
    "            question_strips = psg.split('?')\n",
    "            origin_strips = []\n",
    "            for qs in question_strips:\n",
    "                origin_strips += qs.split('. ')\n",
    "            strips = []\n",
    "            for s in origin_strips:\n",
    "                if s in strips:\n",
    "                    continue\n",
    "                if strips == []:\n",
    "                    strips.append(s)\n",
    "                else:\n",
    "                    if len(s.split()) > 5:\n",
    "                        strips.append(s)\n",
    "                    else:\n",
    "                        strips[-1] += s\n",
    "            final_strips = []\n",
    "            buf = []\n",
    "            for strip in strips:\n",
    "                buf.append(strip)\n",
    "                if len(buf) == num_concatenate_strips:\n",
    "                    final_strips.append(' '.join(buf))\n",
    "                    buf = []\n",
    "            if buf != []:\n",
    "                final_strips.append(' '.join(buf))\n",
    "            return final_strips\n",
    "        elif mode == 'selection':\n",
    "            return [psg]\n",
    "        \n",
    "    def get_relevant_strips(self, strips: List[str], query: str) -> str:\n",
    "        strips_data = []\n",
    "        for p in strips:\n",
    "            if len(p.split()) < 4:\n",
    "                scores = -1.0\n",
    "            else:\n",
    "                input_content = query + \" [SEP] \" + p\n",
    "                inputs = self.tokenizer(input_content, return_tensors = \"pt\", padding = \"max_length\", truncation = True, max_length = self.max_length)\n",
    "                try:\n",
    "                    with torch.no_grad():  \n",
    "                        outputs = self.model(inputs[\"input_ids\"].to(self.device), \n",
    "                                        attention_mask=inputs[\"attention_mask\"].to(self.device))\n",
    "                    scores = float(outputs[\"logits\"].cpu())\n",
    "                except:\n",
    "                    scores = -1.0\n",
    "            strips_data.append((scores, p))\n",
    "        \n",
    "        def take_idx(elem):\n",
    "            return elem[0]\n",
    "        sorted_results = sorted(strips_data, key = take_idx, reverse = True)[:]\n",
    "        filtered_results = [data for data in sorted_results if data[0] > self.filter_threshold]\n",
    "        ctxs = [s[1] for s in filtered_results[:self.top_n]]\n",
    "        \n",
    "        return '; '.join(ctxs)\n",
    "    \n",
    "    def score_and_relevance_flag(self, query: str, docs: List[str]) -> int:\n",
    "        self.model.eval()\n",
    "        \n",
    "        scores = []\n",
    "        for doc in docs:\n",
    "            input_content = query + \" [SEP] \" + doc\n",
    "            inputs = self.tokenizer(input_content, return_tensors = \"pt\", padding = \"max_length\", truncation = True, max_length = self.max_length)\n",
    "            try:\n",
    "                with torch.no_grad():  \n",
    "                    outputs = self.model(inputs[\"input_ids\"].to(self.device), \n",
    "                                    attention_mask=inputs[\"attention_mask\"].to(self.device))\n",
    "                scores.append(float(outputs[\"logits\"].cpu()))\n",
    "            except:\n",
    "                scores.append(-1.0)\n",
    "            \n",
    "        incorrect_flag = 0\n",
    "        \n",
    "        for score in scores:\n",
    "            if score >= self.upper_threshold:\n",
    "                return 2\n",
    "            elif score < self.lower_threshold:\n",
    "                incorrect_flag += 1\n",
    "            \n",
    "        if incorrect_flag == len(docs):\n",
    "            return 0\n",
    "        \n",
    "        return 1\n",
    "    \n",
    "    def web_search_and_filter(self, query: str) -> str:\n",
    "        tavily_client = TavilyClient(api_key = tavily_api)\n",
    "        response = tavily_client.search(query, max_results = 5)\n",
    "        contents = []\n",
    "        for i in range(5):\n",
    "            content = response['results'][i]['content']\n",
    "            contents += self.split_into_passages(content)\n",
    "        final_strips = self.get_relevant_strips(contents, query)\n",
    "        return final_strips\n",
    "    \n",
    "    def C_RAG(self, query: str, docs: List[str]) -> str:\n",
    "        flag = self.score_and_relevance_flag(query, docs)\n",
    "        if flag == 2:\n",
    "            internal_info = []\n",
    "            for p in docs:\n",
    "                internal_info += self.split_into_passages(p)\n",
    "\n",
    "            return self.get_relevant_strips(internal_info, query)\n",
    "        \n",
    "        elif flag == 0:\n",
    "            return self.web_search_and_filter(query)\n",
    "        \n",
    "        else:\n",
    "            internal_info = []\n",
    "            for p in docs:\n",
    "                internal_info += self.split_into_passages(p)\n",
    "                \n",
    "            external_info = self.web_search_and_filter(query)\n",
    "            \n",
    "            return self.get_relevant_strips(internal_info, query) + \"; \" + external_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "1cc78211-96fe-4aa0-bf1d-7209726ae84a",
    "_uuid": "33792a70-4450-47c0-9e7d-1119612f283d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class A_RAG:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        LLM_model: PreTrainedModel,\n",
    "        LLM_tokenizer: PreTrainedTokenizer,\n",
    "        max_iters: int,\n",
    "        maxlen: int,\n",
    "        device: str,\n",
    "        corrective_rag_object\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.LLM_model = LLM_model\n",
    "        self.LLM_tokenizer = LLM_tokenizer\n",
    "        self.max_iters = max_iters\n",
    "        self.device = device\n",
    "        self.max_length = maxlen\n",
    "        self.converter = {'A': 0.0,\n",
    "                         'B': 1.0,\n",
    "                         'C': 2.0}\n",
    "        self.corrector = corrective_rag_object\n",
    "        \n",
    "    def retrieve(self, query:str):\n",
    "        return [\"paris is capital of france\"]\n",
    "    def rerank(self,query: str, docs: List[str]):\n",
    "        return docs\n",
    "    def prompt_template_for_check(self, query: str, answer: str) -> str:\n",
    "        return f'''Evaluate if the answer directly addresses the given question, regardless of factual accuracy:\n",
    "\n",
    "Question: {query}\n",
    "Answer: {answer}\n",
    "\n",
    "Please analyze:\n",
    "1. Does the answer attempt to respond to the specific question asked?\n",
    "2. Are the main points of the question addressed in the answer?\n",
    "3. Is the answer on-topic and relevant to the query?\n",
    "\n",
    "Respond with:\n",
    "- \"YES\" if the answer addresses the question\n",
    "- \"NO\" if the answer is irrelevant or off-topic\n",
    "- Brief explanation of why (1-2 sentences)\n",
    "'''\n",
    "\n",
    "    def get_prompt_for_no_retrieval(self, query: str):\n",
    "        return f'''Provide a clear and comprehensive answer to the following question:\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please follow these guidelines:\n",
    "1. Give a direct, focused answer first\n",
    "2. Provide relevant context and explanations if needed\n",
    "3. Structure the response logically\n",
    "\n",
    "If the question is unclear or needs clarification, please state what needs to be clarified before proceeding.'''\n",
    "\n",
    "    def get_prompt_for_single_retrieval(self, query, docs):\n",
    "        newline = '\\n'\n",
    "        context_sections = []\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            # Format each document with clear separation and reference number\n",
    "            context_sections.append(f\"Reference {i}:\\n{doc}\\n\")\n",
    "    \n",
    "        # Construct a more detailed prompt with clear instructions\n",
    "        prompt = f\"\"\"Question: {query}\n",
    "\n",
    "Relevant Context:\n",
    "{newline.join(context_sections)}\n",
    "\n",
    "Please provide a comprehensive answer based on the context above. Include specific references to support your response when applicable.\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def get_class(self, query: str):\n",
    "        inputs = self.tokenizer(query, return_tensors = \"pt\", padding = \"max_length\", truncation = True, max_length = 512)\n",
    "        # try:\n",
    "        with torch.no_grad():  \n",
    "            outputs = self.model.generate(inputs[\"input_ids\"].to(self.device), \n",
    "                            attention_mask=inputs[\"attention_mask\"].to(self.device), max_new_tokens = self.max_length)\n",
    "        return self.converter[self.tokenizer.decode(outputs[0], skip_special_tokens = True)]\n",
    "        # except:\n",
    "        #     return float(1.0)\n",
    "\n",
    "    def generate(self, prompt: str):\n",
    "        inputs = self.LLM_tokenizer(prompt, return_tensors = \"pt\", padding = \"max_length\", truncation = True, max_length = 512).to(self.device)\n",
    "        # try:\n",
    "        with torch.no_grad():\n",
    "            outputs = self.LLM_model.generate(**inputs, max_new_tokens = self.max_length)\n",
    "        final_response = self.LLM_tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "        return final_response[len(prompt):].strip()\n",
    "        # except:\n",
    "        #     return prompt   \n",
    "            \n",
    "    def check(self, query, answer):\n",
    "        flag = self.generate(self.prompt_template_for_check(query, answer))\n",
    "        if \"yes\" in flag.lower():\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def no_retrieve(self, query: str):\n",
    "        prompt = self.get_prompt_for_no_retrieval(query)\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def single_retrieve(self, query: str):\n",
    "        docs = self.rerank(query, self.retrieve(query))\n",
    "        docs = self.corrector.C_RAG(query, docs)\n",
    "        prompt = self.get_prompt_for_single_retrieval(query, docs)\n",
    "        return self.generate(prompt)\n",
    "        \n",
    "    def multi_retrieve(self, query: str):\n",
    "        for _ in range(self.max_iters):\n",
    "            docs = self.rerank(query, self.retrieve(query))\n",
    "            docs = self.corrector.C_RAG(query, docs)\n",
    "            updated_query = self.get_prompt_for_single_retrieval(query, docs)\n",
    "            answer = self.generate(updated_query)\n",
    "            if(self.check(query, answer)):\n",
    "                break\n",
    "            updated_query = updated_query + answer\n",
    "        return answer\n",
    "\n",
    "    def arag(self, prompt: str):\n",
    "        complexity = self.get_class(prompt)\n",
    "        if complexity == 0.0:\n",
    "            return self.no_retrieve(prompt)\n",
    "        elif complexity == 1.0:\n",
    "            return self.single_retrieve(prompt)\n",
    "        else:\n",
    "            return self.multi_retrieve(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "763c406f-6256-4386-8e71-a2079cdaf867",
    "_uuid": "2a3102b5-33f1-45d1-bd96-e805ed5efdc3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_until_pattern(model, tokenizer, initial_prompt, pattern, max_length=2048):\n",
    "    # Get the EOS token ID\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Encode initial prompt\n",
    "    input_ids = tokenizer.encode(initial_prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    output_tokens = copy.deepcopy(input_ids)\n",
    "    # Create attention mask\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    # Prepare past key values (KV cache)\n",
    "    past_key_values = None\n",
    "    \n",
    "    # Keep track of just the generated text separately\n",
    "    generated_text = \"\"\n",
    "    current_length = input_ids.shape[1]\n",
    "    \n",
    "    while current_length < max_length:\n",
    "        # Generate next token with KV cache\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            \n",
    "            # Get logits and past key values\n",
    "            logits = outputs.logits\n",
    "            past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # Get the last token's prediction\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            next_token_id = torch.argmax(next_token_logits).unsqueeze(0).unsqueeze(0)\n",
    "            output_tokens = torch.cat([output_tokens, next_token_id], dim = -1)\n",
    "            # Check for EOS token\n",
    "            if next_token_id.item() == eos_token_id:\n",
    "                return tokenizer.decode(output_tokens[0], skip_special_tokens = True), False, None\n",
    "            \n",
    "            # Decode the token\n",
    "            next_token_text = tokenizer.decode(\n",
    "                next_token_id[0],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            generated_text += next_token_text\n",
    "            for match in re.finditer(pattern, generated_text, re.DOTALL):\n",
    "                return tokenizer.decode(output_tokens[0], skip_special_tokens = True), True, (match.start(), match.end())\n",
    "            # Update input_ids for next iteration\n",
    "            input_ids = next_token_id\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "            \n",
    "            current_length += 1\n",
    "    return tokenizer.decode(output_tokens[0], skip_special_tokens = True), False, None  # Return if max_length reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "91fddd0b-0d82-4869-8144-5b594f16df7c",
    "_uuid": "2cea5216-429c-46d2-94f0-f6f9bfaf630c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def prompt_template(query: str):\n",
    "    return f\"\"\"Break down the following multi-hop question into a sequence of logically connected single-hop questions, following a chronological order. Each single-hop question should progress towards answering the original question, and the final single-hop question should directly address the original query. The step of final single-hop query addressing original query is of utmost importance. The single-hop queries should not be more than 5 in number\n",
    "\n",
    "Output: Structure your answer in the exact JSON format provided below, replacing placeholders with the actual question breakdown.\n",
    "{{\n",
    "    \"single_hop_queries\": [\n",
    "        \"First single-hop question that helps address part of the original query.\",\n",
    "        \"Second single-hop question based on previous answer.\",\n",
    "        \"...\",\n",
    "        \"Final single-hop question that directly answers the original query.\"\n",
    "    ]\n",
    "}}\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "2e589913-8902-471c-9175-72993c74f022",
    "_uuid": "c0248642-0de7-4b50-99cb-de7796ea3882",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, name: str, description: str, func: Callable):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "\n",
    "    def generate():\n",
    "        pass \n",
    "\n",
    "# class AgentRegistry:\n",
    "#     def __init__(self):\n",
    "#         self.agents: Dict[str, Agent] = {}\n",
    "    \n",
    "#     def register(self, agent: Agent):\n",
    "#         self.agents[agent.name] = agent\n",
    "    \n",
    "#     def get_agent(self, name: str) -> Agent:\n",
    "#         return self.agents.get(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "d9b9f29b-2bd1-496d-893f-253bb48a85a9",
    "_uuid": "98527764-b130-42e4-8eb7-dde1fd04c72c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class QueryDecomposer:\n",
    "    def __init__(self, model, tokenizer, prompt_template) -> List[str]:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.prompt_template = prompt_template\n",
    "        \n",
    "    def generate_sub_queries(self, prompt: str, max_new_tokens: int = 2048) -> List:\n",
    "        full_prompt = self.prompt_template(prompt)\n",
    "        pattern = r'\\{\\s*\"single_hop_queries\"\\s*:\\s*\\[\\s*(?:\"[^\"]*\"\\s*,\\s*)*\"[^\"]*\"\\s*\\]\\s*\\}'\n",
    "        response, flag, po = generate_until_pattern(self.model, self.tokenizer, full_prompt,\n",
    "                             pattern)\n",
    "        response = response[len(full_prompt):].strip()\n",
    "        print(response)\n",
    "        return json.loads(re.findall(pattern, response)[0])['single_hop_queries'] #maybe .replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Core_Memory:\n",
    "    def __init__(self, embedder, device):\n",
    "        self.device = device\n",
    "        index = faiss.IndexFlatL2(len(embedder.embed_query(\"hello world\")))\n",
    "        self.vector_store = FAISS(\n",
    "            embedding_function=embedder,\n",
    "            index=index,\n",
    "            docstore=InMemoryDocstore(),\n",
    "            index_to_docstore_id={},\n",
    "        )\n",
    "        self.memory = []\n",
    "    def add_qna(self, q, a, docs):\n",
    "        self.memory.append((q, a, docs))\n",
    "        doc = Document(page_content=f\"Past sub-query: {q}\\nSub-query answer: {a}\\nDocuments: {docs}\",\n",
    "                      metadata={'id':len(self.memory)})\n",
    "        self.vector_store.add_documents(documents = [doc], #entities inside this list are pushed in vector store as seperate independent docs\n",
    "                                       ids = [len(self.memory)])\n",
    "    def get_relevant_memories(self, query, topk=1):\n",
    "        results = self.vector_store.similarity_search(\n",
    "            query,\n",
    "            k=topk,\n",
    "            filter={},\n",
    "        )\n",
    "        out = [res.page_content for res in results]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "94f38413-f6e4-4b2c-9705-0852201b8b8c",
    "_uuid": "bf4d43a1-852c-400d-86f6-b9c66db967f8",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class AgentStore:\n",
    "    def __init__(self,encoder, tokenizer):\n",
    "        self.encoder = encoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.agentlist = []\n",
    "        self.registry = {}\n",
    "    def add_agent(self, docstring, agent_name):\n",
    "        inputs = self.tokenizer(docstring,return_tensors = 'pt').to(self.encoder.device)\n",
    "        embedding = self.encoder(**inputs)['last_hidden_state'][:,0][0]\n",
    "        embedding = embedding.detach().cpu()\n",
    "        self.registry[len(self.agentlist)] = agent_name\n",
    "        self.agentlist.append(embedding)\n",
    "    def get_agent(self, query):\n",
    "        inputs = self.tokenizer(query,return_tensors = 'pt').to(self.encoder.device)\n",
    "        embedding = self.encoder(**inputs)['last_hidden_state'][:,0][0]\n",
    "        embedding = embedding.detach().cpu()\n",
    "        dot_products = [(i, F.cosine_similarity(embedding, emb, dim=0).item()) for i, emb in enumerate(self.agentlist)]\n",
    "        max_idx, max_dot_product = max(dot_products, key=lambda x: x[1])\n",
    "        return self.registry[max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_template_coder_agent(query):\n",
    "    return f\"\"\"\n",
    "You are a highly skilled AI coding assistant. Please generate Python code that meets the following requirements:\n",
    "\n",
    "Task:\n",
    "{query}\n",
    "\n",
    "Make sure the code is valid and well-commented. Even if code is avaiable in above context, you are supposed to rewrite it below. Provide the code inside the following format:\n",
    "\n",
    "```python\n",
    "# Your generated code here\n",
    "\"\"\"\n",
    "\n",
    "def correction_prompt_coder_agent(response):\n",
    "    return f\"\"\"\n",
    "The following Python code was generated but contains errors or is not executable:\n",
    "\n",
    "{response}\n",
    "\n",
    "Please review and correct the code so that it works as expected. Ensure the fixed code is provided in the same format:\n",
    "\n",
    "```python\n",
    "# Corrected code here\n",
    "\"\"\"\n",
    "\n",
    "def pattern_correction_coder_agent(response):\n",
    "    return f\"\"\"\n",
    "The generated response \\n{response}\\n does not conform to the required format. Ensure the Python code is enclosed within the following format:\n",
    "\n",
    "```python\n",
    "# Correct code format here\n",
    "\"\"\"\n",
    "def pt_math(query):\n",
    "    return f\"\"\"\n",
    "Answer the below given query in twenty to thirty words.\n",
    "Task:\n",
    "{query}\n",
    "You are strictly not allowed to exceed 20-30 words and do not generate any type of code.\n",
    "Answer:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_template_doc(python_code):\n",
    "    return f\"\"\"\n",
    "You are an expert documentation assistant. Given the following Python code, generate its documentation in the JSON format below:\n",
    "\n",
    "JSON Schema:\n",
    "function_info = {{\n",
    "    \"name\": function_name,          # The name of the function.\n",
    "    \"description\": description,     # A brief description of what the function does.\n",
    "    \"parameters\": parameters_dict,  # A dictionary of parameter names and their descriptions.\n",
    "    \"returns\": returns              # A brief description of the return value.\n",
    "}}\n",
    "\n",
    "Python Code:\n",
    "{python_code}\n",
    "\n",
    "Provide only the JSON documentation as the output. Do not include explanations or additional text.\n",
    "\"\"\"\n",
    "def pattern_correction_prompt_template_doc(previous_response):\n",
    "    return f\"\"\"\n",
    "The documentation generated does not match the required JSON schema. Ensure the documentation adheres to the format below:\n",
    "\n",
    "JSON Schema:\n",
    "function_info = {{\n",
    "    \"name\": function_name,          # The name of the function.\n",
    "    \"description\": description,     # A brief description of what the function does.\n",
    "    \"parameters\": parameters_dict,  # A dictionary of parameter names and their descriptions.\n",
    "    \"returns\": returns              # A brief description of the return value.\n",
    "}}\n",
    "\n",
    "The previous response was:\n",
    "{previous_response}\n",
    "\n",
    "Revise the response so it matches the exact JSON schema. Provide only the JSON documentation as the output.\n",
    "\"\"\"\n",
    "\n",
    "def prompt_temp(query, l1, l2, l3):\n",
    "    \"\"\"\n",
    "    Combines the current query with long-term memory (l1), short-term memory (l2),\n",
    "    and database query results (l3) to form a cohesive prompt.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The current query or input from the user.\n",
    "    l1 (str): Long-term memory context or prior conversation context.\n",
    "    l2 (str): Short-term memory from recent subquery context.\n",
    "    l3 (str): Database query results or external retrieved context.\n",
    "\n",
    "    Returns:\n",
    "    str: A prompt that integrates all input components.\n",
    "    \"\"\"\n",
    "    prompt = \"### User Query:\\n\"\n",
    "    prompt += f\"{query}\\n\\n\"\n",
    "\n",
    "    if l1:\n",
    "        prompt += \"### Long-term Context:\\n\"\n",
    "        prompt += f\"{l1}\\n\\n\"\n",
    "\n",
    "    if l2:\n",
    "        prompt += \"### Short-term Context:\\n\"\n",
    "        prompt += f\"{l2}\\n\\n\"\n",
    "\n",
    "    if l3:\n",
    "        prompt += \"### Relevant Data from Database:\\n\"\n",
    "        prompt += f\"{l3}\\n\\n\"\n",
    "\n",
    "    prompt += \"### Generate a cohesive response based on the above information.\"\n",
    "    return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "5275727f-a390-460c-8be5-239e50da9ce8",
    "_uuid": "55e2c465-2f22-4188-98d9-082a28f51540",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class ShortTermMemory:\n",
    "    \"\"\"\n",
    "    Simple BGE-based Short-Term Memory Implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, cross_encoder=None, tokenizer=None, pathwayobj=None):\n",
    "        \"\"\"\n",
    "        Initialize the Short-Term Memory system.\n",
    "        \n",
    "        Args:\n",
    "            cross_encoder: Optional cross-encoder model\n",
    "            tokenizer: Optional tokenizer\n",
    "            pathwayobj: Optional pathway object\n",
    "        \"\"\"\n",
    "        # Model for embedding documents and queries\n",
    "        self.query_embed_model = SentenceTransformer('BAAI/bge-base-en-v1.5')\n",
    "        \n",
    "        # Optional additional components\n",
    "        self.cross_encoder = cross_encoder\n",
    "        self.tokenizer = tokenizer \n",
    "        self.pathwayobj = pathwayobj\n",
    "        \n",
    "        # Embedding instructions\n",
    "        self.DEFAULT_EMBED_INSTRUCTION = \"Represent the document for retrieval: \"\n",
    "        self.DEFAULT_QUERY_INSTRUCTION = (\n",
    "            \"Represent the question for retrieving supporting documents: \"\n",
    "        )\n",
    "        self.DEFAULT_QUERY_BGE_INSTRUCTION_EN = (\n",
    "            \"Represent this question for searching relevant passages: \"\n",
    "        )\n",
    "        \n",
    "        # Storage for queries\n",
    "        self._query_texts = []\n",
    "        self._query_embeddings = []\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Compute document embeddings using a HuggingFace transformer model.\n",
    "        \n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "        \n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        # Prepare texts with embedding instruction\n",
    "        prepared_texts = [\n",
    "            f\"{self.DEFAULT_EMBED_INSTRUCTION}{t.replace('\\n', ' ')}\" \n",
    "            for t in texts\n",
    "        ]\n",
    "        \n",
    "        # Encode texts\n",
    "        embeddings = self.query_embed_model.encode(prepared_texts)\n",
    "        \n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Compute query embeddings using a HuggingFace transformer model.\n",
    "        \n",
    "        Args:\n",
    "            text: The text to embed.\n",
    "        \n",
    "        Returns:\n",
    "            Embeddings for the text.\n",
    "        \"\"\"\n",
    "        # Prepare query with embedding instruction\n",
    "        prepared_text = f\"{self.DEFAULT_QUERY_INSTRUCTION}{text.replace('\\n', ' ')}\"\n",
    "        \n",
    "        # Encode query\n",
    "        embedding = self.query_embed_model.encode(prepared_text)\n",
    "        \n",
    "        return embedding.tolist()\n",
    "\n",
    "    def add_query_text(self, query_text: str):\n",
    "        \"\"\"\n",
    "        Add query text to the short-term memory and compute its embedding.\n",
    "        \n",
    "        Args:\n",
    "            query_text: The query text to add.\n",
    "        \"\"\"\n",
    "        # Add query text\n",
    "        self._query_texts.append(query_text)\n",
    "        \n",
    "        # Compute and store embedding\n",
    "        query_embedding = self.embed_query(query_text)\n",
    "        self._query_embeddings.append(query_embedding)\n",
    "\n",
    "    def get_relevant_short_memory(self, query: str, top_k: int = 3):\n",
    "        \"\"\"\n",
    "        Retrieve most relevant queries from short-term memory.\n",
    "        \n",
    "        Args:\n",
    "            query: The query to find relevant memories for.\n",
    "            top_k: Number of top relevant memories to return.\n",
    "        \n",
    "        Returns:\n",
    "            List of most relevant query texts.\n",
    "        \"\"\"\n",
    "        # If no memories exist, return empty list\n",
    "        if not self._query_texts:\n",
    "            return []\n",
    "        \n",
    "        # Embed the input query\n",
    "        query_embedding = self.embed_query(query)\n",
    "        \n",
    "        # Compute cosine similarities\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        similarities = cosine_similarity(\n",
    "            [query_embedding], \n",
    "            self._query_embeddings\n",
    "        )[0]\n",
    "        \n",
    "        # Get indices of top-k most similar memories\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        # Return corresponding query texts\n",
    "        return [self._query_texts[idx] for idx in top_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "f7f75827-f80e-42d1-8d4f-d356a40d7d09",
    "_uuid": "52026e62-9628-4c2d-885b-d16a2fc43a1b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CodeGenerator(Agent):\n",
    "    def __init__(self,model, tokenizer, prompt_template, correction_prompt_template, pattern_correction_prompt_template):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        # self.arag_obj = arag_obj\n",
    "        self.prompt_template = prompt_template\n",
    "        self.correction_prompt_template = correction_prompt_template\n",
    "        self.pattern_correction_prompt_template = pattern_correction_prompt_template \n",
    "        \n",
    "    def generate(self,prompt):\n",
    "        full_prompt = self.prompt_template(prompt)\n",
    "        pattern = r\"```python\\s+([\\s\\S]+?)\\s+```\"\n",
    "        response, flag, po = generate_until_pattern(self.model, self.tokenizer, full_prompt,pattern)\n",
    "        response = response[len(full_prompt):].strip()\n",
    "        if flag:\n",
    "            try:\n",
    "                match = re.findall(pattern, response)\n",
    "                exec(match[0])\n",
    "                return match[0]\n",
    "            except:\n",
    "                full_prompt = self.correction_prompt_template(response)\n",
    "                response, _ , _ = generate_until_pattern(self.model, self.tokenizer,full_prompt, pattern)\n",
    "                match = re.findall(pattern, response)\n",
    "                if(len(match)==0):\n",
    "                    return response\n",
    "                return match[0]\n",
    "        else:\n",
    "            full_prompt = self.pattern_correction_prompt_template(response)\n",
    "            respone, _ , _ = generate_until_pattern(self.model, self.tokenizer,full_prompt, pattern)\n",
    "            match = re.findall(pattern, response)\n",
    "            if(len(match)==0):\n",
    "                return response\n",
    "            return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentationGenerator(Agent):\n",
    "    def __init__(self, model, tokenizer, prompt_template, pattern_correction_prompt_template):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_template = prompt_template\n",
    "        self.pattern_correction_prompt_template = pattern_correction_prompt_template\n",
    "    def generate(self, prompt):\n",
    "        full_prompt = self.prompt_template(prompt)\n",
    "        pattern = r'```json\\s*({[\\s\\S]*?\"name\"\\s*:\\s*\"[^\"]*\"[\\s\\S]*?\"description\"\\s*:\\s*\"[^\"]*\"[\\s\\S]*?\"parameters\"\\s*:\\s*{[\\s\\S]*?}[\\s\\S]*?\"returns\"\\s*:\\s*\"[^\"]*\"\\s*})\\s*```'\n",
    "        response, flag, po = generate_until_pattern(self.model, self.tokenizer, full_prompt,pattern)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HRManager(Agent):\n",
    "    def __init__(self,model,tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    def generate(self,prompt):\n",
    "        inputs = self.tokenizer(prompt,return_tensors = 'pt').to(self.model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            pad_token_id = self.tokenizer.eos_token_id,\n",
    "            do_sample = False,\n",
    "            max_new_tokens = 2048\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0],skip_special_tokens = True)[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathExpert(Agent):\n",
    "    def __init__(self,model,tokenizer, prompt_template):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_template = prompt_template\n",
    "    def generate(self,prompt):\n",
    "        prompt = self.prompt_template(prompt)\n",
    "        inputs = self.tokenizer(prompt,return_tensors = 'pt').to(self.model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            pad_token_id = self.tokenizer.eos_token_id,\n",
    "            do_sample = False,\n",
    "            max_new_tokens = 2048\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0],skip_special_tokens = True)[len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database:\n",
    "    def __init__(self):\n",
    "        self.rag_client = RAGClient(port=8080)\n",
    "        self.JSON_DIR = \"/workspace/codesearchnet/dataset.json\"\n",
    "        with open(self.JSON_DIR,'r') as f:\n",
    "            self.code_book = json.load(f)\n",
    "    def retrieve(self, query, top_k=1):\n",
    "        retrieved_docs = [self.code_book[docstr] for docstr in self.rag_client.search_documents(query,top_k)]\n",
    "        return retrieved_docs\n",
    "class Database2:\n",
    "    def __init__(self):\n",
    "        self.rag_client = RAGClient(port=8081)\n",
    "    def retrieve(self, query, top_k=1):\n",
    "        return self.rag_client.search_documents(query,top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    query = \"Create a Python script for an embedded temperature monitoring system that reads sensor data from a simulated I2C temperature sensor. The system should continuously monitor temperature readings, calculate a running average over the last 5 readings, and trigger an alert if the temperature exceeds a configurable threshold. The script should include error handling for sensor communication failures and implement a basic logging system. Use the smbus2 library for I2C communication simulation.\"\n",
    "    decomposer = QueryDecomposer(model,tokenizer,prompt_template)\n",
    "    coder_agent = CodeGenerator(model, tokenizer,prompt_template_coder_agent, correction_prompt_coder_agent, pattern_correction_coder_agent)\n",
    "    doc_agent = DocumentationGenerator(model, tokenizer, prompt_template_doc, pattern_correction_prompt_template_doc)\n",
    "    math_expert = MathExpert(model, tokenizer, pt_math)\n",
    "    hr_agent =  HRManager(model, tokenizer)\n",
    "    short_memory = ShortTermMemory() \n",
    "    core_memory = Core_Memory(embedder, device)\n",
    "    database = Database()\n",
    "    hr_database = Database2()\n",
    "    sub_queries = decomposer.generate_sub_queries(query)\n",
    "    sub_queries.append(query)\n",
    "    agent_assigner = AgentStore(encoder, enc_tokenizer)\n",
    "    agent_assigner.add_agent('''The Web Search Agent is a highly efficient tool designed to gather, filter, and summarize information from the internet. It specializes in conducting searches, retrieving relevant data, and providing accurate insights tailored to user needs. Equipped with advanced search algorithms and real-time access to web resources, this agent excels in navigating the vast digital landscape to deliver concise and actionable information.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Conduct targeted searches across the web to retrieve up-to-date information.\n",
    "Summarize and synthesize information from multiple credible sources.\n",
    "Monitor trends, news, and developments in specific domains.\n",
    "Assist with research tasks, including academic, technical, and market research.\n",
    "Evaluate the credibility and relevance of online content.\n",
    "Provide references and citations for the gathered data in a user-friendly format.\n",
    "This agent is particularly useful for users seeking precise and timely information without sifting through excessive data.''', 'math_expert')\n",
    "    agent_assigner.add_agent('''The Coding Agent specializes in programming, software development, and debugging. It is designed to automate the development process, manage version control, and streamline technical workflows. Equipped with deep knowledge of multiple programming languages, frameworks, and best practices, this agent can handle tasks ranging from code generation to system optimization.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Generate, review, and debug code in various programming languages (e.g., Python, JavaScript, C++).\n",
    "Build and integrate APIs, databases, and front-end/back-end systems.\n",
    "Assist in deploying and maintaining software systems on cloud platforms.\n",
    "Implement best practices for security, scalability, and performance.\n",
    "Use machine learning libraries and frameworks to build AI models when required.\n",
    "Automate repetitive coding tasks, version control management, and code documentation.''', 'code_expert')\n",
    "    agent_assigner.add_agent('''The HR Manager Agent focuses on workforce management, talent acquisition, and employee engagement. Designed to emulate the strategic and empathetic aspects of human resource management, this agent ensures smooth operations and maintains a positive organizational culture.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Screen and shortlist candidates for job roles based on predefined criteria.\n",
    "Automate onboarding processes and training programs for new hires.\n",
    "Monitor employee performance and provide tailored development plans.\n",
    "Conduct virtual interviews, manage HR analytics, and track team satisfaction.\n",
    "Address workplace issues and provide data-driven solutions to improve productivity.\n",
    "Maintain compliance with labor laws and organizational policies.''', 'hr_manager')\n",
    "    for sub_query in sub_queries:\n",
    "        assigned_agent = agent_assigner.get_agent(sub_query)\n",
    "        if sub_query == query:\n",
    "            assigned_agent = \"code_expert\"\n",
    "        print(\"query:\",sub_query,\" agent:\",assigned_agent)\n",
    "        if assigned_agent == 'code_expert':\n",
    "            l1 = short_memory.get_relevant_short_memory(sub_query)\n",
    "            l2 = core_memory.get_relevant_memories(sub_query)\n",
    "            l3 = database.retrieve(sub_query)\n",
    "            aug_sub_query = prompt_temp(query, l1, l2, l3)\n",
    "            outputs = coder_agent.generate(aug_sub_query)\n",
    "            doc_files = doc_agent.generate(outputs)\n",
    "            short_memory.add_query_text(f\"Query:{sub_query}\\nAnswer:\\n{doc_files+outputs}\")\n",
    "            core_memory.add_qna(sub_query, outputs, doc_files)\n",
    "        elif assigned_agent == 'math_expert':\n",
    "            l1 = short_memory.get_relevant_short_memory(sub_query)\n",
    "            l2 = []\n",
    "            l3 = []\n",
    "            aug_sub_query = prompt_temp(query, l1, l2, l3)\n",
    "            outputs = math_expert.generate(aug_sub_query)\n",
    "            short_memory.add_query_text(f\"Query:{sub_query}\\nAnswer:\\n{outputs}\")\n",
    "        else:\n",
    "            l1 = short_memory.get_relevant_short_memory(sub_query)\n",
    "            l2 = []\n",
    "            l3 = hr_database.retrieve(sub_query)\n",
    "            aug_sub_query = prompt_temp(query, l1, l2, l3)\n",
    "            outputs = math_expert.generate(aug_sub_query)\n",
    "            short_memory.add_query_text(f\"Query:{sub_query}\\nAnswer:\\n{outputs}\")\n",
    "    print(outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"single_hop_queries\": [\n",
      "        \"What is the Python library for simulating I2C communication?\",\n",
      "        \"How can we create a Python script for an embedded system?\",\n",
      "        \"What is the process for continuously monitoring temperature readings in Python?\",\n",
      "        \"How can we calculate a running average of the last 5 readings in Python?\",\n",
      "        \"What should be done to trigger an alert if the temperature exceeds a configurable threshold?\",\n",
      "        \"How can we implement error handling for sensor communication failures in Python?\",\n",
      "        \"What is a basic logging system in Python and how can we implement it?\",\n",
      "        \"How can we configure a threshold for temperature alerts in the script?\"\n",
      "    ]\n",
      "}\n",
      "query: What is the Python library for simulating I2C communication?  agent: math_expert\n",
      "query: How can we create a Python script for an embedded system?  agent: math_expert\n",
      "query: What is the process for continuously monitoring temperature readings in Python?  agent: math_expert\n",
      "query: How can we calculate a running average of the last 5 readings in Python?  agent: math_expert\n",
      "query: What should be done to trigger an alert if the temperature exceeds a configurable threshold?  agent: math_expert\n",
      "query: How can we implement error handling for sensor communication failures in Python?  agent: math_expert\n",
      "query: What is a basic logging system in Python and how can we implement it?  agent: math_expert\n",
      "query: How can we configure a threshold for temperature alerts in the script?  agent: math_expert\n",
      "query: Create a Python script for an embedded temperature monitoring system that reads sensor data from a simulated I2C temperature sensor. The system should continuously monitor temperature readings, calculate a running average over the last 5 readings, and trigger an alert if the temperature exceeds a configurable threshold. The script should include error handling for sensor communication failures and implement a basic logging system. Use the smbus2 library for I2C communication simulation.  agent: code_expert\n",
      "import time\n",
      "import smbus2\n",
      "from datetime import datetime\n",
      "import logging\n",
      "\n",
      "# I2C bus and address of the temperature sensor\n",
      "I2C_BUS = 1\n",
      "I2C_ADDR = 0x27\n",
      "\n",
      "# Configuration parameters\n",
      "TEMP_THRESHOLD = 30.0  # Change this value to set the temperature threshold\n",
      "SAMPLE_INTERVAL = 1.0  # Interval between temperature readings in seconds\n",
      "LOG_FILE = \"temperature_log.txt\"\n",
      "\n",
      "# Create a logger\n",
      "logging.basicConfig(filename=LOG_FILE, level=logging.INFO,\n",
      "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "def read_temperature():\n",
      "    # Function to read temperature from the sensor\n",
      "    bus = smbus2.SMBus(I2C_BUS)\n",
      "    bus.write_byte_data(I2C_ADDR, 0x00, 0x61)  # Write command to read temperature\n",
      "    data = bus.read_i2c_block_data(I2C_ADDR, 2)  # Read the temperature data\n",
      "\n",
      "    # Convert the data to temperature in Celsius\n",
      "    high = (data[0] << 8) | data[1]\n",
      "    temp = high / 65536.0 * 165.0 - 40.0\n",
      "\n",
      "    return temp\n",
      "\n",
      "def calculate_average(data):\n",
      "    # Function to calculate the running average of the last 5 readings\n",
      "    if len(data) < 5:\n",
      "        return sum(data) / len(data)\n",
      "    else:\n",
      "        return (sum(data[-5:]) + sum(data[:5])) / len(data)\n",
      "\n",
      "def main():\n",
      "    # Main function to continuously monitor temperature readings\n",
      "    data = []\n",
      "    while True:\n",
      "        try:\n",
      "            temp = read_temperature()\n",
      "            data.append(temp)\n",
      "            avg = calculate_average(data)\n",
      "\n",
      "            if temp > TEMP_THRESHOLD:\n",
      "                logger.warning(f\"Temperature alert! Current temperature: {temp}C, Average: {avg}C\")\n",
      "\n",
      "            logger.info(f\"Temperature reading: {temp}C, Average: {avg}C\")\n",
      "\n",
      "            time.sleep(SAMPLE_INTERVAL)\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error reading temperature: {e}\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
